{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KAIP Week 3 - Tutorial 3\n",
    "# Supervised Learning using Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terminology\n",
    "1. Training, validation and testing set\n",
    "2. Hyperparameters\n",
    "3. Cross-validation, Model Selection\n",
    "4. Activation Function:\n",
    "    - sigmoid\n",
    "    - relu \n",
    "5. Loss/ Error/ Cost Function\n",
    "    - Binary Cross Entropy\n",
    "6. Optimization:\n",
    "    - adam optimizer\n",
    "7. Neural Network Classifiers:\n",
    "    - Multilayer Perceptron (MLP)\n",
    "    - Convolutional Neural Network (CNN)\n",
    "    - Recurrent Neural Network (RNN)\n",
    "    - Long Short Term Memory Network (LSTM)\n",
    "8. Padding of sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Sentiment Analysis?\n",
    "Sentiment Analysis is a natural language application where the objective is to predict the underlying intent of the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "In this tutorial, we will analyze the sentiment of film reviews from IMDB dataset, which is already **pre-processed** by Keras:\n",
    "\n",
    "*Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.*\n",
    "\n",
    "Read more here: https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Now, let's train some neural network models using the IMDB data!* 😃\n",
    "\n",
    "Our goal is to learn to **classify** whether the review is POSITIVE or NEGATIVE i.e. supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import models, layers\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, SimpleRNN\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from supervised_nn import *\n",
    "\n",
    "# reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Import Dataset\n",
    "The dataset that we'll be using today is the IMDB dataset, which is already pre-processed by Keras (more to come on data pre-processing in Week 4!)\n",
    "\n",
    "Features contains the representation of the review. Labels contains whether each review was positive or negative.\n",
    "\n",
    "Labels definition:\n",
    "     - 0 = negative review\n",
    "     - 1 = positive review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "n_samples = 25000\n",
    "top_words = 1000\n",
    "inputs, labels = load_imdb(top_words, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Data Exploration\n",
    "**All of the dataset** - Investigate the features of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of samples=' , len(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique categories:\", np.unique(labels))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(inputs))))\n",
    "length = [len(i) for i in inputs]\n",
    "print(\"Average review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**One data sample** - Let's look at what one data sample looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_num = 0\n",
    "describe_review(item_num, inputs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Investigate data item number 10 - what is the original review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you think are the challenges of working with this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Split data into training, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = split_dataset(inputs, labels, 0.2)\n",
    "x_train, y_train, x_val, y_val = split_dataset(x_train, y_train, 0.2)\n",
    "print('Total Number of samples: ', str(len(inputs)))\n",
    "print('Number of training samples: ', str(len(x_train)))\n",
    "print('Number of validation samples: ', str(len(x_val)))\n",
    "print('Number of testing samples: ', str(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The length of reviews varies, so we will pad all sequences so that they are of the same length:\n",
    "max_words = 300\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_val = sequence.pad_sequences(x_val, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can you tell what does 'padding' do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question__: Draw the black-box model for the IMBD sentiment analysis problem. What are the inputs? The ouputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras recipe for Deep Learning\n",
    "1. Model:\n",
    "    - Sequential: A linear stack of layers. Read more here: https://keras.io/getting-started/sequential-model-guide/\n",
    "<br>\n",
    "2. Layers:\n",
    "    - Embeddings: Turns positive integers (indexes) into dense vectors (Pre-processing step).\n",
    "    - Dense: regular densely-connected NN layer.\n",
    "    - Activation: applies an activation function to the input.\n",
    "    - Simple RNN: Fully-connected RNN where the output is to be fed back to input.\n",
    "    - LSTM: Long Short-Term Memory layer - Hochreiter 1997.\n",
    "    - To read more about all the above layers: https://keras.io/layers/core/\n",
    "\n",
    "<img src='workflow.png'>\n",
    "\n",
    "3. Activation Functions: (won't go into too much details but for your info!)\n",
    "    <img src = 'activations.png'>\n",
    "    Source: http://rasbt.github.io/mlxtend/user_guide/general_concepts/activation-functions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simple Multilayer Perceptron (also known as MLP)\n",
    "\n",
    "\"A multilayer perceptron (MLP) is a fully connected neural network, i.e., all the nodes from the current layer are connected to the next layer. A MLP consisting in 3 or more layers: an input layer, an output layer and one or more hidden layers. Note that the activation function for the nodes in all the layers (except the input layer) is a non-linear function.\" Source: https://github.com/rcassani/mlp-example\n",
    "\n",
    "\n",
    "<img src= 'mlp.png'>\n",
    "\n",
    "\n",
    "MLPs are a very powerful too because they allow us to very easily perform non-linear tasks.\n",
    "Let’s begin with a very simple example, from http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/, two curves on a plane. The network will learn to classify points as belonging to one or the other.\n",
    "\n",
    "<img src = 'curves_plane.png'>\n",
    "\n",
    "A linear classifier such as logistic regression, would produce something like that:\n",
    "\n",
    "<img src = 'curves_plane_linear.png'>\n",
    "\n",
    "However, with a non linear classifier, such as the MLP, you get the following\n",
    "\n",
    "<img src = 'curves_plane_nonlinear.png'>\n",
    "\n",
    "Why?? It's all thanks to the hidden layer ! The hidden layer contains a non-linearity (for example the sigmoid from yesterday) that will \"squish\" the data, so that the data becomes linearly separable ! An then the output layer can successfuly take a linear combination and correctly classify the output.\n",
    "\n",
    "<img src = 'curves_plane_squished.png'>\n",
    "\n",
    "We can look at an animation of what happens in a simple MLP with one hidden layer\n",
    "<img src = 'mlp_gif.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = models.Sequential()\n",
    "\n",
    "# Input Layer\n",
    "mlp_model.add(Embedding(top_words, 30, input_length=max_words))\n",
    "\n",
    "# Hidden Layers\n",
    "mlp_model.add(layers.Dense(20, activation = \"relu\"))\n",
    "\n",
    "mlp_model.add(Flatten())\n",
    "\n",
    "# Output Layer\n",
    "mlp_model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# Summarize Model\n",
    "mlp_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Model Fitting or \"Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hyperparameters\n",
    "epochs = 4\n",
    "batch_size = 128\n",
    "\n",
    "results_mlp = mlp_model.fit(x_train, y_train, epochs= epochs, batch_size = batch_size, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Model Evaluation \n",
    "Evaluate the model in terms of accuracy, where accuracy = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_mlp = mlp_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"MLP Accuracy: %.2f%%\" % (scores_mlp[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: In your team, look up two advantages and disadvantages of using MLP? What kind of applications are appropriate for MLP? (10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Convolutional Neural Network (also known as CNN)\n",
    "\n",
    "\n",
    "Convolutional Neural Networks have been a revolution in the field of computer vision, because they can look at \"features\" in the image, and combine them together ( firt they will see lines, then combine into edges, then into high level features like eyes, faces, ...).\n",
    "A big emerging field is feature visualisation, which is very tied to explainability, and opening of the black box. \n",
    "\n",
    "First, how does a CNN look like? \n",
    "\n",
    "<img src = 'convnet_pic.png'>\n",
    "\n",
    "You can see each of the filters performs \"convolutions\", i.e. looks at a small square in the image, and sweeps across the whole input. It is this property that will allow the network to see \"real\" things. Let's take a look at what the CNN sees.\n",
    "\n",
    "<img src = 'layer_viz_colah.png'>\n",
    "\n",
    "image source: https://distill.pub/2017/feature-visualization/ (distill is, in the authors' opinion, one of the best machine learning blogs out there).\n",
    "\n",
    "\n",
    "But it turns out, CNNs are not restricted to images ! Just like we detect patterns in an image, we can use convolutions on sequences of words to recognize meaningul semantic patterns. \n",
    "\n",
    "<img src = 'conv_1D_2D.png'>\n",
    "\n",
    "image source : https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Besides the fact that CNNs are very powerful for extracting meaningful features, they have two other big advanges.\n",
    "- They use mathematical operations called convolutions (hence the name CNN), which can be computed extremely efficiently on graphic cards (GPU) in your computer. This has led GPU manufacturing companies such as Nvidia to grow immensely, and become leaders in the field of AI.\n",
    "- They have much fewer parameters than MLPs, which are \"fully connected\" networks, i.e. each neuron in one layer is connected to each neuron in the next layer. This also allows to train very fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "cnn_model = Sequential()\n",
    "cnn_model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "cnn_model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "cnn_model.add(AveragePooling1D(pool_size=2))\n",
    "cnn_model.add(Flatten())\n",
    "cnn_model.add(Dense(1, activation='sigmoid'))\n",
    "print(cnn_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Model Fitting or \"Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 128\n",
    "\n",
    "cnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_cnn = cnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"CNN Accuracy: %.2f%%\" % (scores_cnn[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Recurrent Neural Networks (also known as RNN)\n",
    "\n",
    "\n",
    "Traditional neural networks do not preserve temporal or sequential information. Recurrent neural networks were then developed in the 1980s, where John Hopfeild discovered Hopfield networks in 1982. <br>\n",
    "\n",
    "\n",
    "<img src='RNN-unrolled.png'> <br>\n",
    "Img reference and excellent resource: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: In your team, research 3 different applications that are RNNs are useful to be used for? What are common features of the data across all applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They come in different forms depending on the application: <br>\n",
    "<img src='rnn_types_1.png'>\n",
    "Img reference and excellent resource: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Can you think of examples for each of the different RNN structures above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our own RNN! \n",
    "\n",
    "### I. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "rnn_model.add(SimpleRNN(20))\n",
    "rnn_model.add(Dense(1, activation='sigmoid'))\n",
    "rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Model Fitting or \"Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=3 \n",
    "batch_size=128\n",
    "\n",
    "rnn_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### IV. Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores_rnn = rnn_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_rnn[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of RNNs that has been thoroughly explored in research is**: LONG TERM DEPENDENCIES!\n",
    "\n",
    "Example: Fill in the blank.\n",
    "- easy task: \"the camel is in the ?”  --> easy to predict desert!\n",
    "- more difficult task: \"“I grew up in the UAE… I speak fluent ?” --> need to look further back to guess that it's Arabic! \n",
    "\n",
    "\n",
    "In theory, RNNs are capable of handling such “long-term dependencies.” However experiments have shown otherwise, which is why LSTMs were developed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Long Short Term Memory Networks (also known as LSTM)\n",
    "\n",
    "It is a VARIANT of RNNs!\n",
    "\n",
    "\"Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997), and were refined and popularized by many people in following work.1 They work tremendously well on a large variety of problems, and are now widely used.\" (colah's blog)\n",
    "\n",
    "\n",
    "- 1) Simple RNN\n",
    "    <img src='rnn_in.png'>\n",
    "\n",
    "\n",
    "- 2) LSTM\n",
    "    <img src='lstm_in.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "lstm_model.add(LSTM(20))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Model Fitting or \"Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Model Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "scores_lstm = lstm_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores_lstm[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: In your team, research the advantages of LSTM over simple RNNs! What are the disadvatages of LSTMs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Linear classifier : Logistic regression (aka a simple benchmark)\n",
    "\n",
    "\n",
    "So far, we have used all the most fancy methods for NLP. But what about a linear classifier, like the ones we used yesterday?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "lr_model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "lr_model.add(Flatten())\n",
    "\n",
    "# Output layer \n",
    "lr_model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "\n",
    "# Print the model summary\n",
    "lr_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What's another way to run logistic regression in Python? *Hint: scikit learn*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Model Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hyperparameters\n",
    "epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "results_lr = lr_model.fit(x_train, y_train, epochs= epochs, batch_size = batch_size, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_lr = lr_model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Logistic Regression Accuracy: %.2f%%\" % (scores_lr[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Benchmark all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [scores_lr[1], scores_mlp[1], scores_rnn[1], scores_lstm[1], scores_cnn[1]]\n",
    "models = ['LR', 'MLP', 'RNN', 'LSTM', 'CNN']\n",
    "plt.plot(models,scores, 'ro', markersize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read more:\n",
    "https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4\n",
    "https://machinelearningmastery.com/predict-sentiment-movie-reviews-using-deep-learning/\n",
    "https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this tutorial, we've explored the IMDB sentiment analysis dataset in NLP. We've developed five different models (deep learning models and a benchmark logistic regression) and we compared their performances. \n",
    "\n",
    "We hope that throughout the tutorial, you gained a high level, intuitive understanding of how these different models works. \n",
    "Besides that, there are two other very important take home messages: \n",
    "- Always try simple models ! As we saw, the linear classifier performs extremely well on this problem!! Of course, this also has to do with the fact that the non linear methods such as CNNs, RNNs, etc, need (a lot) more design and adjustment. Indeed, the best performing networks on this problem have achieved 99% accuracy !! See https://www.kaggle.com/c/word2vec-nlp-tutorial\n",
    "- Data preprocessing (cleaning, but also dimensionality reduction) is absolutely crucial. The IMDB sentiment analysis, in the original Stanfordpaper http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf , painstakingly achieved 87% accuracy. Couple of years later, word embeddings were popularized, and we can achieve this score with a simple linear classifier ! The subject of data pre-processing and preparation will be the subject of week4\n",
    "- Evaluate the uncertainty (confidence intervals, standard deviation, etc) of your metrics using bootstrapping or cross validation to check for statistical significance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
